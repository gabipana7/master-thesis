\documentclass[12pt, twoside]{report}
%\documentclass[12pt]{report}		

%%% CHOOSE THE COLOR SCHEME %%%
% User defined color which will be used throughout the document
% In case you want another color, find its HTML code (without #), a 6-digit string consisting of numbers and letters, then replace it in the brackets
\def\htmlmaincolorcode{607B8B} 
% Analogously, choose the color for shaded equation boxes, in case you wish to use them
\def\htmlshadecolorcode{E4EDF2}
% If you don't want any color and prefer black, just replace the option [customcolor] with [black]
\usepackage[customcolor]{ctp-fpub}

\usepackage{biblatex}
\bibliography{references}


%%% ADD HERE ALL THE PACKAGES YOU USE %%%
% Packages required only for the specific content in this template, remove if not needed
\usepackage{slashed}
\usepackage{dsfont}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{verbatim}


%%% COMMANDS & PACKAGES FOR TIKZIT %%%
%%% Requires tikzit.sty in the main folder, remove if not needed
\usepackage{tikzit}
\definecolor{gaugelink}{HTML}{87608b}
\definecolor{plaquette}{HTML}{607b8b}
\input{tikzit/plaquettes.tikzstyles}
% Package needed to scale .tikz diagrams
\usepackage{tikzscale}
\usepackage{multirow}
\usepackage{caption}
\captionsetup[figure]{font=footnotesize}

\usepackage{subcaption}
\captionsetup{compatibility=false}

\usepackage[toc,page]{appendix}
\usepackage{listings}

% If you want to compile just a part of the thesis use \includeonly
% For example:
% \includeonly{filename2,filename3}


%%% REPLACE THE INFORMATION IN THE BRACKETS %%%
% Information contained in the title page
\def\studentname{Gabriel Tiberiu Pan\u{a}} 
\def\thesistitle{Properties of \\seismic scale-free networks}
% In case of no subtitle, just leave this field empty {}
\def\thesissubtitle{applications for Romania, United States of America, Italy and Japan}
\def\thesistype{Master}
\def\firstsupervisor{Alexandru Nicolin}
\def\firstsupervisorprefix{Conf. Univ. Dr.}
% In case of no second supervsior, just leave these fields empty {}
\def\secondsupervisor{Virgil B\u{a}ran}
\def\secondsupervisorprefix{Prof. Univ. Dr.}
% In case of a third supervisor, well... 
\def\domeniu{Fizic\u{a} Teoretic\u{a} \c{s}i Computa\c{t}ional\u{a} / Theoretical and Computational Physics}
\def\tipteza{Master}

\begin{document}

% Title page
\newgeometry{margin=1in}
\input{title.tex}
\restoregeometry

{\pagestyle{empty}\cleardoublepage}


\tableofcontents
{\pagestyle{empty}\cleardoublepage}
\addtocontents{toc}{\protect\thispagestyle{empty}}

\pagenumbering{arabic}

% Don't include the .tex extension for the file name with include.
\include{intro}
{\pagestyle{empty}\cleardoublepage}
% Add chapters here
\include{complexSystems}
{\pagestyle{empty}\cleardoublepage}
\include{quakeModels}
{\pagestyle{empty}\cleardoublepage}
\include{quakeDatabase}
{\pagestyle{empty}\cleardoublepage}
\include{quakeNetwork}
{\pagestyle{empty}\cleardoublepage}
\include{quakeCorrelations}
{\pagestyle{empty}\cleardoublepage}

\include{conclusions}
{\pagestyle{empty}\cleardoublepage}

% ---------- BIBLIOGRAPHY ------------------%
\printbibliography

{\pagestyle{empty}\cleardoublepage}
% ---------- APPENDIX ------------------%
\begin{appendices}
\chapter{Collecting Databases, Energy Release and Cube Splitting}

\begin{lstlisting}[language=python, frame=single, tabsize=1]  % Start your code-block
import mysql.connector
import pandas as pd
import math

def sqlCollect(condition,side,region,withCubes=True,
		enRel=False):
	mydb = mysql.connector.connect(
	host="",
	user="",
	password="",
	database="",
	auth_plugin='mysql_native_password'
	)

	mycursor = mydb.cursor()

	# Call the database with the set conditions
	mycursor.execute(condition)

	# Append the database to the result object array
	result = mycursor.fetchall()

	# Initialize our lists    
	date= []
	lat = []
	long = []
	dep = []
	mag = []

	# Append the result of sql into our lists
	for i in range(len(result)):
		date.append(result[i][0])
		lat.append(result[i][1])
		long.append(result[i][2])
		dep.append(result[i][3])
		mag.append(result[i][4])
	  
	# Force the results into float type
	lat = [float(item) for item in lat]
	long = [float(item) for item in long]
	dep = [float(item) for item in dep]
	mag = [float(item) for item in mag]
	
	# If withCubes TRUE => collect database and 
	#create cube indexes
	if withCubes:
		# Create cubes indexes for all the dimensions
		# ------------------------------------------

		# Set min max for the dimensions
		minLat = min(lat)
		maxLat = max(lat)
		minLong = min(long)
		maxLong = max(long)
		mindep = min(dep)
		maxdep = max(dep)


		# Find the no. cubes, given the size of the cube in km 
		# Depending on region, one degree lat/long differs:

		# IN VRANCEA 1 deg Lat = 111km / 1 deg Long = 79km
		if region=='Vrancea' or region=='Romania':
			longInKm = 79
		
		# IN CALI 1 deg Lat = 111km / 1 deg Long = 94km
		if region=='California':
			longInKm = 94

		# IN ITALY 1 deg Lat = 111km / 1 deg Long = 84km
		if region=='Italy':
			longInKm = 84

		# IN JAPAN 1 deg Lat = 111km / 1 deg Long = 91km
		if region=='Japan':
			longInKm = 91

		x = round((maxLat-minLat)*111 / side)
		y = round((maxLong-minLong)*longInKm / side)
		z = round((maxdep-mindep) / side)

		# Create cubes indexes for all the dimensions 
		xlat = [math.floor((i-minLat)*x/(maxLat-minLat))+1
		 for i in lat]
		ylong = [math.floor((i-minLong)*y/(maxLong-minLong))+1
		 for i in long]
		zdep = [math.floor((i-mindep)*z/(maxdep-mindep))+1
		 for i in dep ]
		cubeIndex=[]
		# and a general cubeindex for graph formation
		for i in range(len(xlat)):
			cubeIndex.append(int(str(xlat[i])+str(ylong[i])
				+str(zdep[i])))


		# Input coordinates of every cubeIndex
		cubelat=[round(minLat + (side/111)*(i-1+1/2),4)
		 for i in xlat]
		cubelong=[round(minLong + (side/longInKm)*(i-1+1/2),4)
		 for i in ylong]
		cubedep=[round(mindep + (side)*(i-1+1/2),4)
		 for i in zdep]

		# if enRel TRUE, add a column with the energy release 
		if enRel:
			enRel=[math.pow(10, 5.24 + 1.44*i) for i in mag]

			d = {'date':date,'lat':lat,'long':long,
				'dep':dep,'mag':mag, 'enRel':enRel,'x':xlat,
				'y':ylong,'z':zdep,'cubeIndex':cubeIndex,
				'cubelat':cubelat,'cubelong':cubelong,
				'cubedep':cubedep}
			return pd.DataFrame(data=d)


		else:
			# Create DataFrame
			d = {'date':date,'lat':lat,'long':long,
				'dep':dep,'mag':mag,'x':xlat,
				'y':ylong,'z':zdep,'cubeIndex':cubeIndex,
				'cubelat':cubelat,'cubelong':cubelong,
					'cubedep':cubedep}
			return pd.DataFrame(data=d)
	
	# if withCubes==False , do not create cubes 
	else:
	
		if enRel:
			enRel=[math.pow(10, 5.24 + 1.44*i) for i in mag]

			d = {'date':date,'lat':lat,'long':long,'dep':dep,
			     'mag':mag,'enRel':enRel}
			return pd.DataFrame(data=d)
		else:
			d = {'date':date,'lat':lat,'long':long,
				'dep':dep,'mag':mag}
			return pd.DataFrame(data=d)

\end{lstlisting}


\chapter{Network Creation}

\begin{lstlisting}[language=python, frame=single, tabsize=1] 
import datetime
import pandas as pd
import networkx as nx


# Takes the qk DataFrame and returns the qk network
# in networkx graph (G) with attributes:
#(cube dimensions and index of qk with same cIdx)
def graphCreation(qk,withEdgeWeight=False):

	# Create graph -  qk ONE BY ONE
	G = nx.Graph()
	# Iterate through the earthqk checking
	# for magnitude condition
	for i in range(len(qk['cIdx'])):
		# Create a node for this earthquake
		G.add_node(qk['cIdx'][i])
		# check the following earthqk chronologically
		for j in range(i+1, len(qk['cIdx'])):
			# I want only the first occurence (break at the end)
			# create a node for the target quake index
			G.add_node(qk['cIdx'][j])
			
			# Check if you need the edge weights
			if withEdgeWeight==True:
				# WITH EDGE WEIGHT
				# check to see if there is an edge between them
				if G.has_edge(qk['cIdx'][i], qk['cIdx'][j]):
					# we added this one, increase the weight by 1
					G[qk['cIdx'][i]][qk['cIdx'][j]]['weight'] += 1
					break

				else:
					# new edge. add with weight=1
					G.add_edge(qk['cIdx'][i], qk['cIdx'][j], weight=1)
					break
			else:
				# WITHOUT EDGE WEIGHT
				G.add_edge(qk['cIdx'][i], qk['cIdx'][j])
				break

	# Setup network attributes as dictionaries
	# Only dictionaries are supported by networkx
	quake_index = {}
	quake_zDepth = {}
	quake_yLongitude = {}
	quake_xLatitude = {}

	# Turn the dataframe into a dictionary, the indexing qty
	for index, row in qk.iterrows():
		
		# Create a dictionary with cIdx :[list of quake idxs]
		# if the cube has already been indexed
		if row['cIdx'] in quake_index.keys():
			# just add that event in the dictionary
			quake_index[row['cIdx']].append(str(index))
		# if not yet indexed
		else:
			# create that dictionary entry
			quake_index[row['cIdx']]=[]
			# and add that event in the dictionary
			quake_index[row['cIdx']].append(str(index))
			
		# Create dict for each dimension (cIdx : dimension)
		quake_zDepth[row['cIdx']] = str(row['z'])
		quake_yLongitude[row['cIdx']] = str(row['y'])
		quake_xLatitude[row['cIdx']] = str(row['x'])


	nx.set_node_attributes(G, quake_zDepth, 'quake_zDep')
	nx.set_node_attributes(G, quake_yLongitude, 'quake_yLong')
	nx.set_node_attributes(G, quake_xLatitude, 'quake_xLat')
	nx.set_node_attributes(G, quake_index, 'quake_index')


	# Relabel the nodes
	nodeList=[]
	for n in G.nodes():
		nodeList.append(n)

	# Create the mapping : dict with {G node value} : 
	#new value ( from 1 to n = len of G nodes )    
	mapping = {}
	for i in range(len(nodeList)):
		# i+1 to create from 1 to n ( not from 0 ) 
		mapping[nodeList[i]] = i+1

	# Create new graph with relabeled nodes
	G = nx.relabel_nodes(G, mapping)

	return(G)

\end{lstlisting}


\chapter{Connectivity}

\begin{lstlisting}[language=python, frame=single, tabsize=1] 
from quakesNetwork_graphCreation import*
from sqlCollectDatabaseWithCubes import sqlCollect

mag=1
side=5

region = Vrancea
edgeWeight = input(' Edgeweight - True / False : ')

# VRANCEA
if region=='Vrancea':
	condition=(f"SELECT * FROM romplus WHERE 
				`dateandtime`>='1976-01-01 00:00:00'"
		   f" AND `latitude`>=45.2 AND 
		   		`latitude`<=46 AND `longitude`>=26" 
		   f" AND `longitude`<=27 AND `depth`>=50 
		   		AND `depth`<=200"
		   f" AND `magnitude`>={mag}")
		   
# Collect the database and create the graph
quakes = sqlCollect(condition,side,region)
		
# Process graph and its connectivity with edge weight
if edgeWeight=='True':
	G = graphCreation(quakes,withEdgeWeight=True)
		
	# Degree
	degree_dict = dict(G.degree(G.nodes(),weight='weight'))
	connectivity=[]
	for item in degree_dict.values():
		connectivity.append(item)
# Withoud weight
else:
	G = graphCreation(quakes)
			
	# Degree
	degree_dict = dict(G.degree(G.nodes()))
	connectivity=[]
	for item in degree_dict.values():
		connectivity.append(item)
		
# Histogram of connectivity
hist, bins = np.histogram(connectivity,
    bins=round(math.sqrt(len(connectivity))));

# Create the data k,y from the hist and bins
# k = bins centers. Create empty array of the length of hist
k = np.zeros_like(hist)
# Append to it the centers of the bins
for i in range(1,len(bins)):
	k[i-1]=((bins[i]+bins[i-1])/2)

# Check for zeros in the hist list and cut both 
#k and y where the first zero occurs
for i in range(len(hist)):
	if hist[i]==0:
		y=np.array(hist[:i])
		k=np.array(k[:i])
		break
	# If there is no zeros, make y=hist (full data )
	else:
		y=hist
		
# Renormalize the ydata to 1 => P_k
P_k = np.array([float(i)/sum(y) for i in y])
# Compute the mean degree of G : k_mean
k_mean = np.dot(k,P_k)
# Compute the remaining degree distribution q_k
q = np.array([(k[i+1]*P_k[i+1])/k_mean for
	 i in range(len(k)-1)])
# Compute the entropy
H_q = -np.dot(q,np.log(q))


# Compute the power_law fit to our data 
pars, cov = curve_fit(f=power_law,xdata=k,ydata=P_k,
	maxfev=5000)
# Compute the chi_sq  of fit = sum((obs-exp)^2/exp)
chi_squared = np.sum((P_k-power_law(k,*pars))
	**2/power_law(k,*pars))


fig, ax = plt.subplots(1,2,figsize=(15,5))

# The data, scattered
plt.scatter(k,P_k)
plt.set_xscale('log')
plt.set_yscale('log')
# The fit
plt.plot(k,power_law(k,*pars),
		   label=f'$\gamma$={np.round(pars[1],4)}\n
		   $\chi^2$={np.round(chi_squared,4)}',
		   color='red')

# Legend : gamma coefficient of fit and chi_sq
plt.legend(loc='upper right',fontsize=16,frameon=True)

# Title of connectivity distribution ( data + fit )
plt.set_title('cube size = 5 km ')
plt.set_xlabel('connectivity k')
plt.set_ylabel('P(k)')
\end{lstlisting}



\chapter{Motifs}
\section{Mean and Total Energy per Motif }

\begin{lstlisting}[language=python, frame=single, tabsize=1] 
# Calculates the total and mean energy per motif
def totalMeanEnergyMotif(region,side,mag,
		motif,G,qkDataFrame):
    # Open the motifs file
    fileMotif = open(f'motif.txt', 'r')
    linesMotif = fileMotif.readlines()

    # Properly evaluate the Lines to get the Lists
    motifNodes=[]
    for item in linesMotif:
        motifNodes.append(ast.literal_eval(item))


    # Dictionary for setting motif : 
    energyMotif = {}

    # Iterate through the Motifs in our list
    for motifs in motifNodes:
        
        # lists ( 3 elements ) with total and 
        #mean energy in Motif ( per nodes )
        energyInMotif = []
        meanEnergyInMotif = []
        
        # Iterate through the nodes of the motif
        for i in range(len(motifs)):
            
            # List the quakes in the node
            qkInNode = G.nodes[int(motifs[i])]['qk_idx']
            # Set the energy of that node to 0
            energyNode = 0
            
            # Iterathe through the quakes of that node
            for quake in quakesNode:
                # Raise the total energy accumulated
                energyNode += qkDataFrame['enRel'][int(qk)]
            
            energyInMotif.append(energyNode)
            meanEnMotif.append(energyNode/len(quakesInNode))
        
    
        if motif=='triangles':
            energyMotif[str(motifs)]=
            [sum(energyInMotif),sum(meanEnMotif)/3]
        if motif=='squares':
            energyMotif[str(motifs)]=
            [sum(energyInMotif),sum(meanEnMotif)/4]

    return(motifNodes,energyMotif)
\end{lstlisting}

\section{Area of Triangle Motif calculation}

\begin{lstlisting}[language=python, frame=single, tabsize=1] 
# Uses the magnitudesMotif to calculate
# area/totalMag and area/meanMag per motif
def areasInTriangles(motifNodes,energyMotif,G,qk):
    
    # Initialize lists for areas
    areas=[]
    areasWeightTotalMag=[]
    areasWeightMeanMag=[]

    # Points of trianlge: x=lat,y=long,z=depth 
    x=[0,0,0]
    y=[0,0,0]
    z=[0,0,0]

    # Iterate through the motifs 
    for tr in motifNodes:
        for i in range(3):
            x[i] = qk['cubeLat']
            	[int(G.nodes[int(tr[i])]['qk_idx'][0])]
            y[i] = qk['cubeLong']
            	[int(G.nodes[int(tr[i])]['qk_idx'][0])]
            z[i] = qk['cubeDep']
            	[int(G.nodes[int(tr[i])]['qk_idx'][0])]
        
        # The 3 points of the triangle in GEOPY Point
        X=Point(x[0],y[0],z[0])
        Y=Point(x[1],y[1],z[1])
        Z=Point(x[2],y[2],z[2])
        
        flat_distance_a = distance(X[:2], Y[:2]).km 
        flat_distance_b = distance(Y[:2], Z[:2]).km
        flat_distance_c = distance(X[:2], Z[:2]).km
        
        # Introduce the altitude euclidian distance
        a = math.sqrt(flat_distance_a**2 + (X[2] - Y[2])**2)
        b = math.sqrt(flat_distance_b**2 + (Y[2] - Z[2])**2)
        c = math.sqrt(flat_distance_c**2 + (X[2] - Z[2])**2)

        # calculate semiperimeter
        sp = (a+b+c)/2
        
        # Use Heron's formula 
        A = math.sqrt(abs(sp*round(sp-a,4)
        	*round(sp-b,4)*round(sp-c,4)))
        if A < 5:
            continue
        else:

            AWeightTotalMag = A / energyMotif[str(tr)][0]
            areasWeightTotalMag.append(AWeightTotalMag)
            
            AWeightMeanMag = A / energyMotif[str(tr)][1]
            areasWeightMeanMag.append(AWeightMeanMag)
            
    return(areasWeightTotalMag,areasWeightMeanMag)
\end{lstlisting}

\chapter{Paraview Visualization}
\section{VTK}
\begin{lstlisting}[language=python, frame=single, tabsize=1] 
import vtk

def writeObjects(nodeCoords,
                 motifCoords={},
                 edges = [],
                 scalar = [], name = '', power = 1,
                 escalar = [], ename = '', epower = 1,
                 method = 'vtkPolyData',
                 fileout = 'test'):

    # INITIALIZE THE NODES 
    points = vtk.vtkPoints()
    for node in nodeCoords:
        points.InsertNextPoint(node)   

    if motifCoords:
        # INITIALIZE TRIANGLES
        if len(motifCoords[0]) == 3:
            triangles = vtk.vtkCellArray()
            # Initialize the points for the triangle
            for motif in motifCoords:
                triangle = vtk.vtkTriangle()
                keys=[]
                for nodeIdx in motif.keys():
                    keys.append(nodeIdx)

                triangle.GetPointIds().SetId(0,keys[0]-1)
                triangle.GetPointIds().SetId(1,keys[1]-1)
                triangle.GetPointIds().SetId(2,keys[2]-1)
            
                # Append the created triangle to the triangles
                triangles.InsertNextCell(triangle)  


    # INITIALIZE THE EDGES
    if edges:
        line = vtk.vtkCellArray()
        line.Allocate(len(edges))
        for edge in edges:
            line.InsertNextCell(2)
            line.InsertCellPoint(edge[0])
            line.InsertCellPoint(edge[1])   

    if scalar:
        attribute = vtk.vtkFloatArray()
        attribute.SetNumberOfComponents(1)
        attribute.SetName(name)
        attribute.SetNumberOfTuples(len(scalar))
        for i, j in enumerate(scalar): 
            attribute.SetValue(i,j**power)

            
    if escalar:
        eattribute = vtk.vtkFloatArray()
        eattribute.SetNumberOfComponents(1)
        eattribute.SetName(ename)
        eattribute.SetNumberOfTuples(len(edges))
        for i, j in enumerate(escalar): 
            eattribute.SetValue(i,j**epower)


    if method == 'vtkPolyData':
        polydata = vtk.vtkPolyData()
        polydata.SetPoints(points)

        if motifCoords:
            polydata.SetPolys(triangles)
        if edges:
            polydata.SetLines(line)
        if scalar:
            polydata.GetPointData().AddArray(attribute)
        if escalar:
            polydata.GetCellData().AddArray(eattribute)
        if nodeLabel:
            polydata.GetPointData().AddArray(label)
        writer = vtk.vtkXMLPolyDataWriter()
        writer.SetFileName(fileout+'.vtp')
        writer.SetInputData(polydata)
        writer.Write()
\end{lstlisting}


\section{Paraview}
\begin{lstlisting}[language=python, frame=single, tabsize=1] 
import vtk
import networkx as nx
import ast
import numpy as np

# Select the cube side length in km: ( 10 / 5 km)
side = 5
# Select desired magnitude threshold
mag = 4
# Select region
region='Vrancea'

G = nx.read_gexf(f'quakes{region}_{side}km_{mag}mag.gexf')

# Use readlines() to open the 
fileTriangle = open(f'quakes{region}_triangles.txt', 'r')
linesTriangle = fileTriangle.readlines()

# Properly evaluate the Lines to get the Lists
triangleNodes=[]
for item in linesTriangle:
    triangleNodes.append(ast.literal_eval(item))
    
# Graph containing triangles only
H = nx.Graph()
for item in triangleNodes:
    H.add_edge(int(item[0]),int(item[1]))
    H.add_edge(int(item[1]),int(item[2]))
    H.add_edge(int(item[0]),int(item[2]))
    
# Set the triangle attribute = 0 to each edge
nx.set_edge_attributes(G, 0, name='triangle')

# Iterate through our triangle only network edges
for (u,v) in H.edges():
    # Assign to og network edges attribute triangl =1
    G[u][v]['triangle'] = 1
    
# Get attributes that go into VTK function

# X Y Z coords
lat =[]
long = []
depth =[]
for n in G.nodes():
    lat.append(int(G.nodes[n]['quake_xLatitude']))
    long.append(int(G.nodes[n]['quake_yLongitude']))
    depth.append(int(G.nodes[n]['quake_zDepth']))

minLat = min(lat)
maxLat = max(lat)
minLong = min(long)
maxLong = max(long)
minDepth = min(depth)
maxDepth = max(depth)
maxDim = max(maxLat,maxLong,maxDepth)


coords=[]
for n in G.nodes():
    coords.append([ np.float32(round((int(G.nodes[n]
    	  ['quake_xLatitude'])- minLat)*(maxLat/max)
    	    /(maxLat-minLat),3)),
        np.float32(round((int(G.nodes[n]
          ['quake_yLongitude'])- minLong)*(maxLong/maxDim)
            /(maxLong-minLong),3)),
        np.float32(round((int(G.nodes[n]
          ['quake_zDepth'])- minDepth)*(maxDepth/maxDim)
            /(maxDepth-minDepth),3))])
  
# Degree of nodes edges    
degree = [d for n, d in G.degree()]

# Weight of edges
weights = []
for (i,j) in G.edges():
    weights.append(G[i][j]['weight'])
    
# Triangle quality of edges
triangles = []
for (i,j) in G.edges():
    triangles.append(G[i][j]['triangle'])   
    
from writeNodesEdges import writeObjects

writeObjects(nodeCoords=coords,
             edges=G.edges(),
             scalar=degree, name='degree',
             scalar2=weights, name2='weight',
             escalar2=triangles, ename2='triangle',
#             nodeLabel=nodeLabel,
             fileout=f'network_triangles')
\end{lstlisting}


\chapter{Temporal AutoCorrelations}

\begin{lstlisting}[language=python, frame=single, tabsize=1] 
import arviz as az
import matplotlib
import numpy as np
import datetime

from datetime import datetime  
from datetime import timedelta 


import matplotlib.pyplot as plt
import math
import collections

from statsmodels.graphics.tsaplots import plot_acf
from sqlCollectDatabaseWithCubes import sqlCollect
from quakesNetwork_graphCreation import*

region='California'
magMin=2
magMax=3.5
side=5
                
condition=(f"SELECT * FROM california WHERE 
`dateandtime`>='1984-01-01 00:00:00'"
           f" AND `depth`>=0 AND `magnitude`>={magMin} 
AND `magnitude`<={magMax}" 
           f" AND (`magtype` LIKE 'l' 
OR `magtype` LIKE 'w')")

quakes = sqlCollect(condition,side,region,enRel=True)

timeWindow = max(quakes['date']) - min(quakes['date'])
deltas = [quakes['date'][i+1]-quakes['date'][i]
 for i in range(len(quakes['date'])-1)]
dt = max(deltas)

delta = timedelta(days=1) + dt
windows = round(timeWindow/delta)

interval = {}
for i in range(windows):
    interval[i] = [quakes['date'][0] +
     i*delta,quakes['date'][0] + (i+1)*delta]
     
     
quakesList = {}
enInt = dict.fromkeys(interval.keys(),0)
for i in range(len(interval)):
    quakesList[i] =list(set(quakes.index[interval[i][0]<=
     quakes['date']].tolist()).intersection
     (quakes.index[quakes['date'] <
      interval[i][1]].tolist()))
    for k in quakesList[i]:
        enInt[i] += quakes['energyRelease'][k]

data = list(energyInterval.values())
values = np.array(data)

f = plt.figure()
ax = f.add_subplot(111)
plot_acf(values,lags=len(energyInterval)-1,ax=ax)

plt.xlabel(r'k (lags)')
plt.ylabel(r'C(k)')

\end{lstlisting}

\end{appendices}



% ---------- DECLARATION ------------------%
%\include{declaratie}

\end{document}
